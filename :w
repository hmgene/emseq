input="bigdata/2025-04-11/merged.tsv"
output="results/2025-05-08/anova"
odir=${output%/*}
mkdir -p $odir 
o=$odir/README.md

cutn(){
local tmp=`mktemp -d`;
cat $1 > $tmp/a
Rscript <( echo '
    library(data.table)
    cols=strsplit("'$2'",",")[[1]];
    tt=fread("'$tmp/a'"); 
    fwrite(tt[,..cols],"'$tmp/b'",sep="\t",na = "NA",quote=F)
')
tail -n+2 $tmp/b 
}

cutn ${output}_multi.tsv chr,start,end,min_pval,gW_10,gY_10 | awk -v OFS="\t" '$4<0.05{ b=int($2/10)*10;print $1,b,b+10,$5;}' | sort -k1,1 -k2,3n > gW.bedGraph 
cutn ${output}_multi.tsv chr,start,end,min_pval,gW_10,gY_10 | awk -v OFS="\t" '$4<0.05{ b=int($2/10)*10;print $1,b,b+10,$6;}' | sort -k1,1 -k2,3n > gY.bedGraph 

target_genes=( Hic2 Lin28b Igf2bp2 Igf2bp3 and Hmga2 )
grep -wf <( echo ${target_genes[@]} | tr " " "\n" ) data/gene.bed | sort -k1,1 -k2,3n |\
tee >( awk -v OFS="\t" '{ print $1, $2-10000000,$3+10000000,$4,$5,$6;}' > target_region.bed) |\
closestBed -d -a stdin -b <( cutn ${output}_multi.tsv chr,start,end,min_pval | sort -k1,1 -k2,3n )

cat data/gene.bed | sort -k1,1 -k2,3n |\
windowBed -w 10000 -a stdin -b <( cutn ${output}_multi.tsv chr,start,end,min_pval | awk '$4<0.05' | sort -k1,1 -k2,3n )

exit


. ./src/stat.sh;

run-anova(){
    for b in 1 10 50;do
        anova $input $b 1 > ${output}_${b}bp.tsv
    done
}

run-multibin(){
    Rscript <( echo '
    ## order by bin _\\d+bp
    input=strsplit("results/2025-05-08/anova_1bp.tsv,results/2025-05-08/anova_10bp.tsv,results/2025-05-08/anova_50bp.tsv",",")[[1]]
    output="'$output_multi'"# output="results/2025-05-08/anova_multi"
    library(data.table)

    cn = c("chr","strand","start","end") ## last two 
    cn1 = c("pval","gW","gY","perc")
    tt=NULL;
    for( i in input){
        bz=sub(".*_(\\d+)bp.*", "\\1", i) 
        tmp= fread(i); j=c(cn,grep(paste(cn1,collapse="|"),names(tmp),value=T)); tmp=tmp[,..j]; tmp[, start := as.integer(start)]; tmp[, end := as.integer(end)]
        setkeyv(tmp, cn)
        setnames(tmp,c(cn,paste0(j[(length(cn)+1):(length(j))],"_",bz)))
        if( is.null(tt) ){ tt=tmp;
        }else{ 
            tt= foverlaps(tt,tmp,type="any",nomatch=NA); 
            tt[, start := NULL]; tt[, end := NULL]
            setnames(tt, gsub("^i\\.", "", names(tt)))
        }
    }
    fwrite(tt,paste0(output,".tsv"),sep="\t")

    pval_cols <- grep("^pval_", names(tt), value = TRUE)
    tt[, min_pval := do.call(pmin, c(.SD, list(na.rm = TRUE))), .SDcols = pval_cols]
    m=as.matrix(tt[min_pval < 0.05, grep("perc",names(tt),value=T),with=F])
    m=t(scale(t(m)));
    m[is.na(m)]=0;
    gbin=sub(".+_(\\d+)$","\\1",colnames(m))
    gsam=sub("^([^_]+).+","\\1",colnames(m))

    library(ComplexHeatmap)
    png(file=paste0(output,"_heatmap.png"))
    Heatmap(m,use_raster=F,row_names_gp=gpar(fontsize=6),column_names_gp=gpar(fontsize=6),column_split=gsam)
    dev.off();

    png(paste0(output,"_corbysample.png"))
    Heatmap(cor(m),row_names_gp=gpar(fontsize=6),column_names_gp=gpar(fontsize=6),column_split=gsam,row_split=gsam)
    dev.off();
    png(paste0(output,"_corbybin.png"))
    Heatmap(cor(m),row_names_gp=gpar(fontsize=6),column_names_gp=gpar(fontsize=6),column_split=gbin,row_split=gbin)
    dev.off();

    ## strandness
    y=tt[min_pval<0.05 , if (uniqueN(strand) > 1) .SD, by = .(chr, start, end)]
    y1=dcast(y, chr + start + end ~ strand, value.var = c("gW_1","gW_10","gW_50","gY_1","gY_10","gY_50"), fun.aggregate = mean)
    Heatmap(y1[,4:ncol(y1)],column_split=sub(".+_(\\d+)_[+-]","\\1",colnames(y1)[4:ncol(y1)]))
    ')
}


exit



library(cluster)
library(dplyr)
library(tidyr)
library(stringr)
library(tibble)



library(dplyr)
library(tidyr)
library(stringr)
library(cluster)

# Step 1: Convert the matrix to a long format with sample metadata
df_long <- as.data.frame(m) %>%
  pivot_longer(cols = everything(), names_to = "sample", values_to = "value") %>%
  mutate(
    group = str_extract(sample, "^[^_]+"),                   # e.g., "E18pt5"
    replicate = str_extract(sample, "(?<=_)[0-9]+(?=_)"),    # e.g., "1"
    bin_size = str_extract(sample, "perc_\\d+")              # e.g., "perc_50"
  )


results <- df_wide %>%
  group_by(group, bin_size) %>% summarise()
  summarize(silhouette_score = compute_sil(cur_data()), .groups = "drop")

print(results)




# Example: convert your matrix to a data frame (if needed)
# df <- as.data.frame(your_matrix)

# Get long-form data
df_long <- as.data.frame(m) %>%
  rownames_to_column("feature_id") %>%  # if rownames are meaningful
  pivot_longer(-feature_id, names_to = "sample", values_to = "value") %>%
  mutate(
    group = str_extract(sample, "^[^_]+"),
    replicate = str_extract(sample, "(?<=_)[0-9]+(?=_)"),
    bin_size = str_extract(sample, "perc_\\d+")
  )

# Now pivot wider so each sample is a row, each feature is a column
df_wide <- df_long %>%
  pivot_wider(names_from = feature_id, values_from = value)

# Compute silhouette score for each group+bin+replicate combo
compute_sil <- function(df_group) {
  df_feat <- df_group %>% select(-sample, -group, -replicate, -bin_size)
  if (nrow(df_feat) < 2) return(NA)  # need at least 2 samples
  # Dummy clusters: assume each replicate is its own cluster
  # OR: apply real clustering (e.g., kmeans) if you have ground truth
  km <- kmeans(df_feat, centers = 2)
  d <- dist(df_feat)
  s <- silhouette(km$cluster, d)
  mean(s[, "sil_width"])
}

results <- df_wide %>%
  group_by(group, bin_size) %>%
  summarise(silhouette_score = compute_sil(cur_data()), .groups = "drop")

print(results)

    
')

       

# Clean and rename
result <- tt_joined[, .(chr, start = i.start, end = i.end, pval_1 = pval_10, pval_10 = i.pval_10)]
    
    tt = foverlaps(tt,tt1, by.x = c("chr", "start", "end"), type = "within", nomatch = NA)


')

exit




    pvals[is.na(pvals)]=1

    i= pvals < 0.05
    df=cbind(tt[,1:4],p,pvals)
    write.table(df,file=paste0(o,"_anova.tsv"),sep="\t",row.names=F,col.names=T,quote=F)

    library(ggplot2)
    library(reshape2)
    p_clean <- t(scale(t(p[i,])))  # or use: na.omit(p)
    p_clean$id <- 1:nrow(p_clean)
    meth_long <- melt(p_clean, id.vars = "id", variable.name = "Sample", value.name = "Methylation")
    ggplot(meth_long, aes(x = Sample, y = Methylation, fill = Sample)) +
      geom_violin(trim = FALSE, scale = "width") +
      theme_minimal() +
      labs(title = "Methylation Distribution per Sample", x = "Sample", y = "Methylation Level") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))

    ## clustyering
    m=p[i,]
    m=t(scale(t(m)))
    h =Heatmap(m)
    row_dend <- row_dend(h)
    clusters <- cutree(as.hclust(row_dend), k = 4)

##intersectBed -b data/cpgIslandExt.bed -a <( tail -n+2 results/2025-04-30/filt_anova.tsv | awk '$(NF)<0.05' ) |cut -f 1-4 > results/2025-04-30/incpgi.bed 
x=read.table("results/2025-04-30/incpgi.bed",header=F)
colnames(x)=c("chr","start","end","strand")
x$CpG=T
y=merge(df,x,all.x=T)
y$CpG[is.na(y$CpG)]=F
    right_anno=rowAnnotation(CpG= y[i,]$CpG)
    top_anno = HeatmapAnnotation(foo = anno_boxplot(m, height = unit(2, "cm")))
    png(file=paste0(o,"_heatmap.png"))
    Heatmap(m,top_annotation= top_anno,right_annotation=right_anno,row_split=clusters,show_row_names=F,cluster_columns=F,column_split=g)
    dev.off()

    for (k in unique(clusters)) {
      d <- df[i,][clusters == k, , drop = FALSE]
      write.table(d, file = paste0(o,"_cluster", k, ".tsv"), sep = "\t", row.names = F, col.names = T, quote = F)
    }
'
}
pval=0.05
n=`tail -n+2 $odir/filt_anova.tsv | awk -v p=$pval '$(NF)<p' | wc -l`
N=`tail -n+2 $odir/filt_anova.tsv | wc -l `;
h=$odir/filt_heatmap.png;
echo '## Update : '`date`'
- Improved sensitivity in detecting '$n' / '$N' significant CpG sites (Pval <'$pval')
- Identified unbiased methylation patterns without applying prefiltering (previously required a 25% difference threshold)

## Results
- ANOVA results : [anova](/'$odir/filt_anova.tsv')
- Pval < '$pval' significant sites

| Heatmap | 
| :-: |
| ![hm](/'$h') |

| cluster_id | table |
| :-: | :-: |' > $o
for i in 1 2 3 5;do
    echo "| cluster_$i | [$i](/$odir/filt_cluster$i.tsv) |" >> $o
done

git add -A
git commit -am wooutlier
git push

exit
anno(){
i=filtered_anova.tsv
o=${i%.tsv}_anno
annotatePeaks.pl $i mm10 -go ${o}_go -annStats $o.stats > $o.tsv 
for i in 1 2 3 4;do
    continue;
    #i=merged_anova_pval005_cluster$i.tsv
    i=filtered_anova_cluster$i.tsv
    o=${i%.tsv}_anno
    annotatePeaks.pl $i mm10 -go ${o}_go -annStats $o.stats > $o.tsv 
done
}

bam2tsv(){
. ../../src/meth.sh
for f in *.bam;do
    n=${f%.dedu*}
    #bisbam2meth $f $n $n.tsv
    #chr1	4337982	4337982	+	11	8	3
done
}

cpg(){
    input=(
    2-year_1.tsv	2-year_3.tsv	E18pt5_1.tsv	E18pt5_3.tsv	Week4_1.tsv	Week4_3.tsv	
    2-year_2.tsv	2-year_4.tsv	E18pt5_2.tsv	E18pt5_4.tsv	Week4_2.tsv	Week4_4.tsv	
    )

    for f in ${input[@]};do
        n=${f%.tsv}
        #chr	start	end	strand	coverage	numCs	numTs
        tail -n+2 $f |  awk -v n=$n -v OFS="\t" '{print $1,$2,$2+1,n,$5,$6;}' |\
        intersectBed -a ../../data/cpgIslandExt.bed -b stdin -wa -wb |\
        sort -k1,1 -k2,3n | groupBy -g 1,2,3,8 -c 9,10, -o sum,sum
    done  |  awk '{print $1":"$2"-"$3"\t"$4"\t"($6/$5);}' | ca rc2mat -  > cpg.tsv
}

input=(
    E18pt5_1.tsv E18pt5_2.tsv E18pt5_3.tsv E18pt5_4.tsv
    Week4_1.tsv Week4_2.tsv Week4_3.tsv Week4_4.tsv
    2-year_1.tsv 2-year_2.tsv 2-year_3.tsv 2-year_4.tsv
)
. ../../src/util.sh;
#merge ${input[@]}  > merged.tsv

